The paper "Attention Is All You Need" introduces the Transformer, a novel neural network architecture for sequence transduction tasks, such as machine translation.  The Transformer eschews recurrence and convolutions, relying entirely on an attention mechanism (specifically, a scaled dot-product attention and multi-head attention) to capture global dependencies between input and output sequences.  This allows for significantly greater parallelization during training, resulting in faster training times and improved translation quality.

Key objectives were to create a more parallelizable and efficient architecture for sequence transduction, improving upon existing recurrent and convolutional models.  The authors achieved this by replacing recurrent and convolutional layers with self-attention mechanisms.

The conclusions demonstrate the Transformer's superiority. Experiments on the WMT 2014 English-to-German and English-to-French translation tasks showed the Transformer achieving state-of-the-art BLEU scores, surpassing previous best results (including ensembles) with significantly less training time.  The paper also explores variations in the Transformer's architecture, analyzing the impact of different hyperparameters on performance.  The authors conclude by highlighting the potential of attention-based models for various tasks beyond machine translation.